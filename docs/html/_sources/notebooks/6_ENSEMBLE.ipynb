{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelo Combinado (Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa combinamos os melhores modelos. No caso como não foi possível fazer um treinamento com a base mista, excluí dessa combinação o modelo EfficientNetV2 visto que nos teste ele não generalizou e preveu todos como “fake”, errando assim todos os vídeos reais.\n",
    "\n",
    "Dessa forma o nosso “Ensemble” contará com o modelo Mesonet e o melhor modelo de decomposição de espectro. Como o modelo de decomposição é composto de duas etapas, decomposição da imagem em um array e classificação com um modelo tradicional (Regressão logística ou SVM), tínhamos algumas variações para comparar. Optei pelo modelo que utiliza um SVM com os parametros (SVC(C=6.37, kernel='rbf', gamma=0.86)) que obteve os melhores resultados.\n",
    "\n",
    "A combinação foi feita através de uma função que roda os dois modelos, captura as predições e calcula aplicando como peso a média das acurácias que obtivemos nos testes anteriores.\n",
    "\n",
    "Esse cálculo é aplicado para cada predição (face extraída do vídeo), e o resultado final é a média.\n",
    "A combinação foi feita através de uma função que roda os dois modelos, captura as predições e calcula aplicando como peso a média das acurácias que obtivemos nos testes anteriores.\n",
    "\n",
    "$$\n",
    "\\text{Prob.Deepfake} = \\frac{{(\\text{prob.model1} \\times \\text{peso1}) + (\\text{prob.model2} \\times \\text{peso2})}}{{\\text{peso1} + \\text{peso2}}}\n",
    "$$\n",
    "\n",
    "Esse cálculo é aplicado para cada predição (face extraída do vídeo), e o resultado final é a média de todas as predições.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment('Projeto Aplicado XPE - Detector de Deep Fake')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "from scipy.interpolate import griddata\n",
    "from glob import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Tamanho a imagem e canais\n",
    "image_dimensions = {'height':256, 'width':256, 'channels':3}\n",
    "\n",
    "# Classe do classificador\n",
    "class Classifier:\n",
    "    def __init__():\n",
    "        self.model = 0 \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)  \n",
    "    def fit(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "    def get_accuracy(self, x, y):\n",
    "        return self.model.test_on_batch(x, y)\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "# Rede Mesonet usando o Classificador\n",
    "class Meso4(Classifier):\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer,\n",
    "                           loss = 'mean_squared_error',\n",
    "                           metrics = ['accuracy'])\n",
    "    \n",
    "    def init_model(self): \n",
    "        x = Input(shape = (image_dimensions['height'],\n",
    "                           image_dimensions['width'],\n",
    "                           image_dimensions['channels']))\n",
    "        \n",
    "        x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "        \n",
    "        x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "        \n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "        \n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "\n",
    "        return Model(inputs = x, outputs = y)\n",
    "    \n",
    "# Utilização de pesos já treinados\n",
    "meso = Meso4()\n",
    "meso.load('../src/models/Meso4_DF.h5')\n",
    "\n",
    "def modelo_meso4():\n",
    "    dataGenerator = ImageDataGenerator(rescale=1./255)\n",
    "    directory = '../data/interim/'\n",
    "    generator = dataGenerator.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(256, 256),\n",
    "        class_mode=None,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    frame = []\n",
    "    real = []\n",
    "    fake = []\n",
    "    for i in range(10):\n",
    "        X = generator.next()\n",
    "        pred = meso.predict(X)[0][0]\n",
    "        frame.append(i)\n",
    "        real.append(pred)\n",
    "        fake.append(1-pred)\n",
    "    resultado_real = sum(real)/len(real)\n",
    "    resultado_fake = 1 - resultado_real\n",
    "    return (frame,real,fake,resultado_real,resultado_fake)\n",
    "\n",
    "# Modelo de análise de Espectro:\n",
    "def azimuthalAverage(image, center=None):\n",
    "    y, x = np.indices(image.shape)\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "    r = np.hypot(x - center[0], y - center[1])\n",
    "    ind = np.argsort(r.flat)\n",
    "    r_sorted = r.flat[ind]\n",
    "    i_sorted = image.flat[ind]\n",
    "    r_int = r_sorted.astype(int)\n",
    "    deltar = r_int[1:] - r_int[:-1]\n",
    "    rind = np.where(deltar)[0]\n",
    "    nr = rind[1:] - rind[:-1]\n",
    "    csim = np.cumsum(i_sorted, dtype=float)\n",
    "    tbin = csim[rind[1:]] - csim[rind[:-1]]\n",
    "    radial_prof = tbin / nr\n",
    "    return radial_prof\n",
    "modelo_espec = pickle.load(open('../src/models/model_espectro.pkl','rb'))\n",
    "\n",
    "def modelo_analise_de_espectro():\n",
    "    epsilon = 1e-8\n",
    "    N = 300\n",
    "    number_iter = 10\n",
    "    psd1D_total = np.zeros([number_iter, N])\n",
    "    label_total = np.zeros([number_iter])\n",
    "    cont = 0\n",
    "    real = []\n",
    "    fake = []\n",
    "    for face in glob('../data/interim/faces/*jpg'):\n",
    "        img = cv2.imread(face,0)\n",
    "        f = np.fft.fft2(img)\n",
    "        fshift = np.fft.fftshift(f)\n",
    "        fshift += epsilon\n",
    "        magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "        psd1D = azimuthalAverage(magnitude_spectrum)\n",
    "        points = np.linspace(0,N,num=psd1D.size) \n",
    "        xi = np.linspace(0,N,num=N)\n",
    "        interpolated = griddata(points,psd1D,xi,method='cubic')\n",
    "        interpolated /= interpolated[0]\n",
    "        psd1D_total[cont,:] = interpolated             \n",
    "        label_total[cont] = 1\n",
    "        cont+=1     \n",
    "    pred = modelo_espec.predict(psd1D_total)\n",
    "    real = list(pred)\n",
    "    fake = list(1-pred)\n",
    "    resultado_real = sum(real)/len(real)\n",
    "    resultado_fake = 1 - resultado_real\n",
    "    return (real,fake,resultado_real,resultado_fake)\n",
    "\n",
    "# função que combina os modelos e calcula o resultado ponderado\n",
    "def roda_modelo():\n",
    "    modelo1 = modelo_meso4()\n",
    "    frame = modelo1[0]\n",
    "    modelo2 = modelo_analise_de_espectro()\n",
    "    peso1 = 0.616\n",
    "    peso2 = 0.707\n",
    "    peso_total = peso1+peso2\n",
    "    real_modelo1 =[]\n",
    "    real_modelo2 =[]\n",
    "    fake_modelo1 =[]\n",
    "    fake_modelo2 =[]\n",
    "    real = []\n",
    "    fake = []\n",
    "    for i in modelo1[1]: real_modelo1.append(i*peso1)\n",
    "    for i in modelo2[0]: real_modelo2.append(i*peso2)\n",
    "    for i in modelo1[2]: fake_modelo1.append(i*peso1)\n",
    "    for i in modelo2[1]: fake_modelo2.append(i*peso2)\n",
    "    for i in [x + y for x, y in zip(real_modelo1, real_modelo2)]: real.append(i/peso_total)\n",
    "    for i in [x + y for x, y in zip(fake_modelo1, fake_modelo2)]: fake.append(i/peso_total)\n",
    "    resultado_real = sum(real)/len(real)\n",
    "    resultado_fake = 1 - resultado_real\n",
    "    return (frame,real,fake,resultado_real,resultado_fake)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Teste do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset CELEBDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "dataset = 'celebdf'\n",
    "dataframe_faces= pd.read_csv('../data/processed/dataset_'+dataset+'/metadados_faces.csv', sep=';', dtype=str)\n",
    "dataframe_faces_teste = dataframe_faces[dataframe_faces['particao']=='teste'].reset_index(drop=True)\n",
    "# Predição\n",
    "teste_ensemble_label = []\n",
    "teste_ensemble_proba = []\n",
    "for i in dataframe_faces_teste['face']:\n",
    "    path_interim = i.replace('processed/dataset_celebdf/','interim/faces/').replace('real_face/','').replace('fake_face/','')\n",
    "    shutil.copy(i,path_interim)\n",
    "    pred = roda_modelo()\n",
    "    teste_ensemble_label.append(pred[3].round())\n",
    "    teste_ensemble_proba.append(pred[3].round(4))\n",
    "    os.remove(path_interim) \n",
    "dataframe_faces_teste['test_pred_label'] = teste_ensemble_label\n",
    "dataframe_faces_teste['test_pred'] = teste_ensemble_proba\n",
    "dataframe_faces_teste.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captura de resultados:\n",
    "previsoes = dataframe_faces_teste['test_pred_label'].astype(int)\n",
    "y = dataframe_faces_teste['label'].astype(int)\n",
    "prob_previsao = dataframe_faces_teste['test_pred']\n",
    "processamento = 'Ensemble_'+dataset\n",
    "with mlflow.start_run(run_name=processamento):\n",
    "    report = classification_report(y, previsoes, output_dict=True)\n",
    "    acuracia = accuracy_score(y, previsoes)\n",
    "    mlflow.log_metric('accuracy', acuracia)\n",
    "    mlflow.log_metric('precision_0', report['0']['precision'])\n",
    "    mlflow.log_metric('recall_0', report['0']['recall'])\n",
    "    mlflow.log_metric('f1-score_0', report['0']['f1-score'])\n",
    "    mlflow.log_metric('precision_1', report['1']['precision'])\n",
    "    mlflow.log_metric('recall_1', report['1']['recall'])\n",
    "    mlflow.log_metric('f1-score_1', report['1']['f1-score'])\n",
    "    auc = roc_auc_score(y, prob_previsao)\n",
    "    mlflow.log_metric('roc_auc',auc)\n",
    "    matriz_confusao = confusion_matrix(y,previsoes)\n",
    "    mlflow.log_metric('0_True_matrix' ,matriz_confusao[0][0])\n",
    "    mlflow.log_metric('0_False_matrix', matriz_confusao[0][1])\n",
    "    mlflow.log_metric('1_False_matrix',matriz_confusao[1][0])\n",
    "    mlflow.log_metric('1_True_matrix',matriz_confusao[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Faceforensics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "dataset = 'faceforensics'\n",
    "dataframe_faces= pd.read_csv('../data/processed/dataset_'+dataset+'/metadados_faces.csv', sep=';', dtype=str)\n",
    "dataframe_faces_teste = dataframe_faces[dataframe_faces['particao']=='teste'].reset_index(drop=True)\n",
    "# Predição\n",
    "teste_ensemble_label = []\n",
    "teste_ensemble_proba = []\n",
    "for i in dataframe_faces_teste['face']:\n",
    "    path_interim = i.replace('processed/dataset_faceforensics/','interim/faces/').replace('real_face/','').replace('fake_face/','')\n",
    "    shutil.copy(i,path_interim)\n",
    "    pred = roda_modelo()\n",
    "    teste_ensemble_label.append(pred[3].round())\n",
    "    teste_ensemble_proba.append(pred[3].round(4))\n",
    "    os.remove(path_interim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captura de resultados\n",
    "dataframe_faces_teste['test_pred_label'] = teste_ensemble_label\n",
    "dataframe_faces_teste['test_pred'] = teste_ensemble_proba\n",
    "previsoes = dataframe_faces_teste['test_pred_label'].astype(int)\n",
    "y = dataframe_faces_teste['label'].astype(int)\n",
    "prob_previsao = dataframe_faces_teste['test_pred']\n",
    "processamento = 'Ensemble_'+dataset\n",
    "with mlflow.start_run(run_name=processamento):\n",
    "    report = classification_report(y, previsoes, output_dict=True)\n",
    "    acuracia = accuracy_score(y, previsoes)\n",
    "    mlflow.log_metric('accuracy', acuracia)\n",
    "    mlflow.log_metric('precision_0', report['0']['precision'])\n",
    "    mlflow.log_metric('recall_0', report['0']['recall'])\n",
    "    mlflow.log_metric('f1-score_0', report['0']['f1-score'])\n",
    "    mlflow.log_metric('precision_1', report['1']['precision'])\n",
    "    mlflow.log_metric('recall_1', report['1']['recall'])\n",
    "    mlflow.log_metric('f1-score_1', report['1']['f1-score'])\n",
    "    auc = roc_auc_score(y, prob_previsao)\n",
    "    mlflow.log_metric('roc_auc',auc)\n",
    "    matriz_confusao = confusion_matrix(y,previsoes)\n",
    "    mlflow.log_metric('0_True_matrix' ,matriz_confusao[0][0])\n",
    "    mlflow.log_metric('0_False_matrix', matriz_confusao[0][1])\n",
    "    mlflow.log_metric('1_False_matrix',matriz_confusao[1][0])\n",
    "    mlflow.log_metric('1_True_matrix',matriz_confusao[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset DFDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "dataset = 'dfdc'\n",
    "dataframe_faces= pd.read_csv('../data/processed/dataset_'+dataset+'/metadados_faces.csv', sep=';', dtype=str)\n",
    "dataframe_faces_teste = dataframe_faces[dataframe_faces['particao']=='teste'].reset_index(drop=True)\n",
    "# Predição\n",
    "teste_ensemble_label = []\n",
    "teste_ensemble_proba = []\n",
    "for i in dataframe_faces_teste['face']:\n",
    "    path_interim = i.replace('processed/dataset_dfdc/','interim/faces/').replace('real_face/','').replace('fake_face/','')\n",
    "    shutil.copy(i,path_interim)\n",
    "    pred = roda_modelo()\n",
    "    teste_ensemble_label.append(pred[3].round())\n",
    "    teste_ensemble_proba.append(pred[3].round(4))\n",
    "    os.remove(path_interim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captura de resultados\n",
    "dataframe_faces_teste['test_pred_label'] = teste_ensemble_label\n",
    "dataframe_faces_teste['test_pred'] = teste_ensemble_proba\n",
    "previsoes = dataframe_faces_teste['test_pred_label'].astype(int)\n",
    "y = dataframe_faces_teste['label'].astype(int)\n",
    "prob_previsao = dataframe_faces_teste['test_pred']\n",
    "processamento = 'Ensemble_'+dataset\n",
    "with mlflow.start_run(run_name=processamento): \n",
    "    report = classification_report(y, previsoes, output_dict=True)\n",
    "    acuracia = accuracy_score(y, previsoes)\n",
    "    mlflow.log_metric('accuracy', acuracia)\n",
    "    mlflow.log_metric('precision_0', report['0']['precision'])\n",
    "    mlflow.log_metric('recall_0', report['0']['recall'])\n",
    "    mlflow.log_metric('f1-score_0', report['0']['f1-score'])\n",
    "    mlflow.log_metric('precision_1', report['1']['precision'])\n",
    "    mlflow.log_metric('recall_1', report['1']['recall'])\n",
    "    mlflow.log_metric('f1-score_1', report['1']['f1-score'])\n",
    "    auc = roc_auc_score(y, prob_previsao)\n",
    "    mlflow.log_metric('roc_auc',auc)\n",
    "    matriz_confusao = confusion_matrix(y,previsoes)\n",
    "    mlflow.log_metric('0_True_matrix' ,matriz_confusao[0][0])\n",
    "    mlflow.log_metric('0_False_matrix', matriz_confusao[0][1])\n",
    "    mlflow.log_metric('1_False_matrix',matriz_confusao[1][0])\n",
    "    mlflow.log_metric('1_True_matrix',matriz_confusao[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_df_proj",
   "language": "python",
   "name": "venv_df_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
